{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ДЗ2 Зиновьева Ольга\n",
    "\n",
    "Я решила сравнить 4 pos-тегера: stanza, natasha, spacy и pymorphy\n",
    "\n",
    "В приложенных материалах есть: корпус текстов в формате txt (просто предложения), размеченный корпус в формате json (он с приколом, там ключ токена - это индекс символа его начала), json-ы c положительными и отрицательными отзывами из прошлого дз."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install stanza"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install natasha"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m spacy download ru_core_news_md"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import json\n",
    "import stanza\n",
    "import spacy\n",
    "from string import punctuation\n",
    "from natasha import Doc, NewsMorphTagger, NewsEmbedding, Segmenter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "import random\n",
    "from typing import List, Union, Tuple, Dict, Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "morph = MorphAnalyzer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stanza.download('ru')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Про то, какую разметку я выбрала. 3 из 4 теггеров делают теги conllu, а pymorphy - opencoepora. И я попыталась сделать так, чтобы отличающиеся теги могли сводиться к чему-то единому. Полностью я убирала местоименные наречия со значением числа и числительные, потому что в conllu они все размечались как num, а в opencorpora как разные части речи.\n",
    "\n",
    "В итоге, у меня разметка включает существительные + местоименные существительные (NOUN), прилагательные + местоименные прилагательные (ADJ), глаголы + причастия + деепричастия (VERB), наречия + частицы (ADV), предлоги (ADP) и союзы (CONJ).\n",
    "\n",
    "Я подумала, что для нашей конечной цели получения n-грамм определенного pos формата объединять местоимения с сущ и прил корректнее, чем их убирать совсем."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "transl_conllu = {\n",
    "    'PRON': 'NOUN',\n",
    "    'PROPN': 'NOUN',\n",
    "    'DET': 'ADJ',\n",
    "    'PART': 'ADV',\n",
    "    'AUX': 'VERB',\n",
    "    'CCONJ': 'CONJ',\n",
    "    'SCONJ': 'CONJ'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "transl_opencorpora = {\n",
    "    'PREP': 'ADP',\n",
    "    'INFN': 'VERB',\n",
    "    'NPRO': 'NOUN',\n",
    "    'GRND': 'VERB',\n",
    "    'ADJF': 'ADJ',\n",
    "    'ADJS': 'ADJ',\n",
    "    'PRCL': 'ADV',\n",
    "    'ADVB': 'ADV',\n",
    "    'PRTS': 'VERB',\n",
    "    'PRTF': 'VERB',\n",
    "    'PRED': 'ADV'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "with open('corpora.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "with open('corpora_with_tags.json', 'r', encoding='utf-8') as f:\n",
    "    corpora = json.loads(f.read())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Прогоняем через stanza"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppln = stanza.Pipeline('ru', processors='tokenize,pos')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "for sent in ppln(text).to_dict():\n",
    "    for item in sent:\n",
    "        token_id = str(item['start_char'])\n",
    "        if token_id in corpora:\n",
    "            token = item['text'].lower().strip(punctuation)\n",
    "            if corpora[token_id]['text'] == token:\n",
    "                corpora[token_id]['stanza_pos'] = item['upos'] if item['upos'] not in transl_conllu else transl_conllu[item['upos']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Прогоняем через natasha"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "doc = Doc(text)\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "segmenter = Segmenter()\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "for token in doc.tokens:\n",
    "        token_id = str(token.start)\n",
    "        if token_id in corpora:\n",
    "            token_text = token.text.lower().strip(punctuation)\n",
    "            if corpora[token_id]['text'] == token_text:\n",
    "                corpora[token_id]['natasha_pos'] = token.pos if token.pos not in transl_conllu else transl_conllu[token.pos]\n",
    "            else:\n",
    "                print(token_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Прогоняем через spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_md\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "for token in nlp(text):\n",
    "    token_id = str(token.idx)\n",
    "    if token_id in corpora:\n",
    "        token_text = token.text.lower().strip(punctuation)\n",
    "        if corpora[token_id]['text'] == token_text:\n",
    "            corpora[token_id]['spacy_pos'] = token.pos_ if token.pos_ not in transl_conllu else transl_conllu[token.pos_]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ну и pymorphy. Справедливости ради, токенизация тут от nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "for token_edges in tokenizer.span_tokenize(text.lower()):\n",
    "    start, end = token_edges\n",
    "    token_id = str(start)\n",
    "    if token_id in corpora:\n",
    "        parsed = morph.parse(text[start:end])[0]\n",
    "        token_text = text[start:end].lower().strip(punctuation)\n",
    "        if corpora[token_id]['text'] == token_text and parsed.tag.POS is not None:\n",
    "            corpora[token_id]['pymorphy_pos'] = str(parsed.tag.POS) if str(parsed.tag.POS) not in transl_opencorpora else transl_opencorpora[str(parsed.tag.POS)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Собираю все это в датафрейм просто для удобства. Тут можно просто поделить кол-во строк, где разметка pos-теггера совпала с ручной и поделить на длину всего датафрейма (потому что оставшиеся - это как раз либо токены, для которых теггер не выдал рез-ты, либо где выдал неправильный рез-т)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "corpora_df = pd.DataFrame(corpora.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy для stanza"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9552238805970149"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpora_df[corpora_df['stanza_pos'] == corpora_df['pos']]) / len(corpora_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для наташи"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8557213930348259"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpora_df[corpora_df['natasha_pos'] == corpora_df['pos']]) / len(corpora_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9054726368159204"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpora_df[corpora_df['spacy_pos'] == corpora_df['pos']]) / len(corpora_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "И pymorphy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8955223880597015"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpora_df[corpora_df['pymorphy_pos'] == corpora_df['pos']]) / len(corpora_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Лидер, очевидно, stanza"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь про выбранные паттерны. Честно говоря, я немного схитрила и просто почекала ручками, какие чаще всего паттерны попадают хотя бы в один из словарей) Еще проверяла для триграмм, но там скорее были какие-то устойчивые выражения без особых паттернов, поэтому чисто биграммы взяла. В целом, выбранные паттерны довольно логичные (не + глагол скорее для отрицательных и для \"не могла оторваться\" в положительных (люблю подглядывать), прилагательные и туда и туда и всякие утсойчивые выражения предлог + сущ тоже и туда и туда)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "formats = {'не VERB', 'ADP NOUN', 'ADJ NOUN'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "with open('pos.json', 'r', encoding='utf-8') as f:\n",
    "    positive = json.loads(f.read())\n",
    "\n",
    "with open('neg.json', 'r', encoding='utf-8') as f:\n",
    "    negative = json.loads(f.read())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "pos = random.sample(sorted(positive), len(negative))\n",
    "neg = negative"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "test_sample = {}\n",
    "for samp in random.sample(sorted(pos), 15):\n",
    "    test_sample[samp] = 'pos'\n",
    "for samp in random.sample(sorted(neg), 15):\n",
    "    test_sample[samp] = 'neg'\n",
    "\n",
    "pos_sample = {samp for samp in pos if samp not in test_sample}\n",
    "neg_sample = {samp for samp in neg if samp not in test_sample}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def get_tokens_and_pattern_bigrams(sample: List[str]) -> List[List[Union[str, Tuple[str]]]]:\n",
    "    \"\"\" Функция для токенизации + выбора n-грамм, соответствующих паттернам\n",
    "    \"\"\"\n",
    "    grams = []\n",
    "    for text in sample:\n",
    "        text_tokenized = tokenizer.tokenize(text.lower())\n",
    "        # Добавляю просто токены в рез-т\n",
    "        grams.append([gr for gr in text_tokenized])\n",
    "        # Очень страшный кусок, но тут я просто прогоняю склеенный токенизованный текст (чтобы без пунктуации) через\n",
    "        # stanza, беру перевеленные из conllu теги или \"не\", и делаю из этих последовательностей тегов биграммы\n",
    "        POS = [\n",
    "            p for p in ngrams(\n",
    "                [\n",
    "                    (\n",
    "                     token['upos'] if token['upos'] not in transl_conllu else transl_conllu[token['upos']]\n",
    "                    ) if token['text'] != 'не' else 'не'\n",
    "                 for token in ppln(' '.join(text_tokenized)).to_dict()[0]\n",
    "                 ],\n",
    "            2)\n",
    "        ]\n",
    "        bigrams = [t for t in ngrams(text_tokenized, 2)]\n",
    "        if len(POS) == len(bigrams):\n",
    "            # Добавляю к токенам биграмму, если у нее pos-паттер нужный\n",
    "            grams[-1].extend([gr for i, gr in enumerate(bigrams) if ' '.join(POS[i]) in formats])\n",
    "    return grams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "pos_grams = get_tokens_and_pattern_bigrams(pos)\n",
    "neg_grams = get_tokens_and_pattern_bigrams(neg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def make_frequency_dicts(grams: List[List[Union[str, Tuple[str]]]]) -> Dict[Union[str, Tuple[str]], int]:\n",
    "    \"\"\" Функция для создания частотных словарей по массиву текстов в виде токенов и n-грамм нужных паттернов\n",
    "    \"\"\"\n",
    "    freq_dict = {}\n",
    "    for text in grams:\n",
    "        for token in text:\n",
    "            freq_dict.setdefault(token, 0)\n",
    "            freq_dict[token] += 1\n",
    "    return freq_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "pos_freq = make_frequency_dicts(pos_grams)\n",
    "neg_freq = make_frequency_dicts(neg_grams)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def make_set_of_unique_tonality_tokens(\n",
    "        this_tonality: Dict[Union[str, Tuple[str]], int],\n",
    "        different_tonality: Dict[Union[str, Tuple[str]], int],\n",
    "        freq_threshold: int\n",
    ") -> Set[Union[str, Tuple[str]]]:\n",
    "    \"\"\" Функция для получения множества токенов и биграмм тональности A, которые не присутствуют в отзывах тональности B\n",
    "    \"\"\"\n",
    "    return set(\n",
    "        [key for key, item in this_tonality.items()\n",
    "         if key not in different_tonality\n",
    "         and item > freq_threshold]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "pos_unique = make_set_of_unique_tonality_tokens(pos_freq, neg_freq, 2)\n",
    "neg_unique = make_set_of_unique_tonality_tokens(neg_freq, pos_freq, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "test_tokenized = get_tokens_and_pattern_bigrams(test_sample)\n",
    "test_tonality = list(test_sample.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def calc_accuracy(\n",
    "        test_texts: List[List[Union[str, Tuple[str]]]],\n",
    "        test_labels: List[str],\n",
    "        pos: Set[Union[str, Tuple[str]]],\n",
    "        neg: Set[Union[str, Tuple[str]]]\n",
    ") -> float:\n",
    "    \"\"\" Функция, которая считает accuracy модели определения тональности для тестового сэмпла на основании уникальных\n",
    "     для pos и neg отзывов токенов и n-грамм\n",
    "     \"\"\"\n",
    "    true_guess = 0\n",
    "\n",
    "    for i, text in enumerate(test_texts):\n",
    "        score = 0\n",
    "        for token in text:\n",
    "            if token in pos:\n",
    "                score += 1\n",
    "            elif token in neg:\n",
    "                score -= 1\n",
    "        true_guess += (test_labels[i] == ('pos' if score > 0 else 'neg' if score < 0 else 'neu'))\n",
    "\n",
    "    return true_guess / len(test_texts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8333333333333334"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(test_tokenized, test_tonality, pos_unique, neg_unique)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь оставляем только токены, без n-грамм"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "test_only_tokens = [[token for token in sent if type(token) == str] for sent in test_tokenized]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "pos_unique_only_tokens = {token for token in pos_unique if type(token) == str}\n",
    "neg_unique_only_tokens = {token for token in neg_unique if type(token) == str}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(test_only_tokens, test_tonality, pos_unique_only_tokens, neg_unique_only_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "На самом деле потестила несколько раз, тк у меня же выборки рандомные. Большинство раз с n-граммами было чуть лучше (максимум на 0.1), и только пару раз были абсолютно одинаковые результаты. Я уверена, что если увеличить тональные словари и кол-во паттернов, будет лучше. В такие моменты начинаешь ценить нейронки"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
