{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ДЗ1 Зиновьева Ольга\n",
    "\n",
    "Я решила парсить отзывы на книги на [Литресе](https://www.litres.ru). Для честности брала именно книги, а не аудиокниги (исключая вероятность, что в отзывах на аудиокниги внимание может уделяться голосу диктора)\n",
    "\n",
    "Этот скрипт будет запускаться только:\n",
    "- не в колабе! он не дружит с селениумом\n",
    "- если у вас есть firefox на компе\n",
    "\n",
    "Эти условия жесткие, поэтому в репо я положила json-ы с крауленными отзывами. Думаю, их названия довольно понятные) Дальше по коду найдите блок с чтением из json-ов и начинайте с него) А если сами запускаете краулинг, то игнорируйте этот блок\n",
    "\n",
    "Почему я юзаю pymorphy3, а не pymorphy2? Потому что у меня на компе стоит 11 питон, а он с pymorphy2 не дружит. [Вот на stackoverflow про это](https://ru.stackoverflow.com/questions/1479188/Почему-не-работает-пакет-pymorphy2-на-python-3-11)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pymorphy3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install selenium"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pymorphy3\n",
    "import json\n",
    "from typing import List, Set, Dict, Optional"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T11:49:11.883195500Z",
     "start_time": "2023-09-19T11:49:11.878675500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    \"\"\" Универсальная функция для краулинга странички по ссылке\n",
    "    \"\"\"\n",
    "    dr =  webdriver.Firefox()\n",
    "    dr.get(url)\n",
    "    soup = BeautifulSoup(dr.page_source)\n",
    "    time.sleep(2)\n",
    "    dr.quit()\n",
    "    time.sleep(3)\n",
    "    return soup"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:11:05.515300300Z",
     "start_time": "2023-09-21T17:11:05.504261400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "План такой:\n",
    "\n",
    "Я стартую с жанровой странички (потому что на главной не нашла ссылок на жанровые странички).\n",
    "Дальше иду по жанрам и краулю книжки с их первой странички. И для каждой книги краулю первую страницу с отзывами.\n",
    "\n",
    "Почему везде только первые страницы? Потому что на литресе промотка страницы сопровождается изменением query, а не пути. А селениум, как и requests, с query не дружат)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "soup = get_soup('https://www.litres.ru/genre/legkoe-chtenie-201583/')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:11:32.270938Z",
     "start_time": "2023-09-21T17:11:07.470577700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "main_url = 'https://www.litres.ru'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:11:34.984457900Z",
     "start_time": "2023-09-21T17:11:34.969419700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для сбора отзывов использую множества, чтобы исключить возможность повторения (в случае, если книга относится к 2 жанрам)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "positive = set()\n",
    "negative = set()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:11:38.344662500Z",
     "start_time": "2023-09-21T17:11:38.316525200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [2:40:08<00:00, 640.58s/it] \n"
     ]
    }
   ],
   "source": [
    "for genre in tqdm(soup.find('div', {'class': 'AllGenres-module__genres-wrapper'}).find_all('a')[1:16]):\n",
    "    genre_soup = get_soup(main_url + genre['href'])\n",
    "\n",
    "    href_blocks = genre_soup.find_all('a', {'class': \"bookname descr-no_js\"})\n",
    "    # Почему-то на разных страницах разные классы у блоков с ссылками. (Кто это писал вообще?)\n",
    "    if not href_blocks:\n",
    "        href_blocks = genre_soup.find_all('a', {'class': \"art__name__href\"})\n",
    "\n",
    "    # Список ссылок на страницы книг с 1 страницы жанра\n",
    "    hrefs = [main_url + block['href'] + 'otzivi/' for block in href_blocks if 'audiobook' not in block['href']]\n",
    "\n",
    "    for href in hrefs:\n",
    "        book_soup = get_soup(href)\n",
    "        for review in book_soup.find_all('article', {'class': 'Comment-module__comment'}):\n",
    "            stars = review.find_all('div', {'class': 'Stars-modules__star Stars-modules__orangeTheme'})\n",
    "\n",
    "            # Считаем цветные звездочки, если отзыв со звездочками. Решила, что 4+ - позитивный, 2- - негативный\n",
    "            if len(stars):\n",
    "                rate = 0\n",
    "                for star in stars:\n",
    "                    if star.find('path', {'class': \"Stars-modules__svgPath Stars-modules__svgPathActive\"}):\n",
    "                        rate += 1\n",
    "                text = review.find('div', {'class': 'Comment-module__reviewText'}).text\n",
    "                if rate < 3:\n",
    "                    negative.add(text)\n",
    "                elif rate > 3:\n",
    "                    positive.add(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:51:50.251954600Z",
     "start_time": "2023-09-21T17:11:41.488000600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "with open('pos.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(list(positive), ensure_ascii=False))\n",
    "\n",
    "with open('neg.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(list(negative), ensure_ascii=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:24.796700900Z",
     "start_time": "2023-09-21T19:52:24.781035700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вот отсюда надо начинать, если вы не запускаете краулинг. А если не запускаете, то не выполняйте этот блок!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "with open('pos.json', 'r', encoding='utf-8') as f:\n",
    "    positive = json.loads(f.read())\n",
    "\n",
    "with open('neg.json', 'r', encoding='utf-8') as f:\n",
    "    negative = json.loads(f.read())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:27.668604100Z",
     "start_time": "2023-09-21T19:52:27.654471900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Положительных - 1562 ; отрицательных - 173\n"
     ]
    }
   ],
   "source": [
    "print('Положительных -', len(positive), '; отрицательных -', len(negative))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:41.942574900Z",
     "start_time": "2023-09-21T19:52:41.929310200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Положительных прямо намного больше, тк мы брали везде первые странички, а там всегда все с более высоким рейтингом. Поэтому берем рандомный сэмпл положительных по длине отрицательных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "pos = random.sample(sorted(positive), len(negative))\n",
    "neg = negative"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:48.668484800Z",
     "start_time": "2023-09-21T19:52:48.664474700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "test_sample = {}\n",
    "for samp in random.sample(sorted(pos), 15):\n",
    "    test_sample[samp] = 'pos'\n",
    "for samp in random.sample(sorted(neg), 15):\n",
    "    test_sample[samp] = 'neg'\n",
    "\n",
    "pos_sample = {samp for samp in pos if samp not in test_sample}\n",
    "neg_sample = {samp for samp in neg if samp not in test_sample}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:50.329621400Z",
     "start_time": "2023-09-21T19:52:50.324570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:52.518781800Z",
     "start_time": "2023-09-21T19:52:52.514797500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [],
   "source": [
    "def tokenize_several_texts(sample: Set[str]) -> List[List[str]]:\n",
    "    \"\"\" Функция для токенизации сэмпла текстов\n",
    "    \"\"\"\n",
    "    return [tokenizer.tokenize(text.lower()) for text in sample]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:54.184710500Z",
     "start_time": "2023-09-21T19:52:54.171610400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "def make_frequency_dicts(tokenized_texts: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\" Функция для создания частотных словарей по массиву токенизованных текстов\n",
    "    \"\"\"\n",
    "    freq_dict = {}\n",
    "    for text in tokenized_texts:\n",
    "        for token in text:\n",
    "            freq_dict.setdefault(token, 0)\n",
    "            freq_dict[token] += 1\n",
    "    return freq_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:54.678719400Z",
     "start_time": "2023-09-21T19:52:54.675647500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [],
   "source": [
    "def make_set_of_unique_tonality_tokens(\n",
    "        this_tonality: Dict[str, int],\n",
    "        different_tonality: Dict[str, int],\n",
    "        freq_threshold: int,\n",
    "        stop_words:Optional[Set[str]]=set()\n",
    ") -> Set[str]:\n",
    "    \"\"\" Функция для получения множества токенов тональности A, которые не присутствуют в отзывах тональности B\n",
    "    \"\"\"\n",
    "    return set(\n",
    "        [key for key, item in this_tonality.items()\n",
    "         if key not in different_tonality\n",
    "         and item > freq_threshold\n",
    "         and key not in stop_words]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:55.599221500Z",
     "start_time": "2023-09-21T19:52:55.570778600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "pos_tokenized = tokenize_several_texts(pos_sample)\n",
    "neg_tokenized = tokenize_several_texts(neg_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:56.334548Z",
     "start_time": "2023-09-21T19:52:56.315866900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [],
   "source": [
    "pos_frequency = make_frequency_dicts(pos_tokenized)\n",
    "neg_frequency = make_frequency_dicts(neg_tokenized)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:57.803483500Z",
     "start_time": "2023-09-21T19:52:57.787369100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "pos_unique = make_set_of_unique_tonality_tokens(pos_frequency, neg_frequency, 2)\n",
    "neg_unique = make_set_of_unique_tonality_tokens(neg_frequency, pos_frequency, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:52:58.422631800Z",
     "start_time": "2023-09-21T19:52:58.417575400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Токенизируем также тестовые тексты"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [],
   "source": [
    "test_tokenized = tokenize_several_texts(test_sample.keys())\n",
    "test_tonality = list(test_sample.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:53:02.266450600Z",
     "start_time": "2023-09-21T19:53:02.258776900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "def calc_accuracy(test_texts: List[List[str]], test_labels: List[str], pos: Set[str], neg: Set[str]) -> float:\n",
    "    \"\"\" Функция, которая считает accuracy модели определения тональности для тестового сэмпла на основании уникальных для pos и neg отзывов токенов\"\"\"\n",
    "    true_guess = 0\n",
    "\n",
    "    for i, text in enumerate(test_texts):\n",
    "        score = 0\n",
    "        for token in text:\n",
    "            if token in pos:\n",
    "                score += 1\n",
    "            elif token in neg:\n",
    "                score -= 1\n",
    "        true_guess += (test_labels[i] == ('pos' if score > 0 else 'neg' if score < 0 else 'neu'))\n",
    "\n",
    "    return true_guess / len(test_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:53:02.796730900Z",
     "start_time": "2023-09-21T19:53:02.792148100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Это результат для tokenized + нижнерегистрного текста"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(test_tokenized, test_tonality, pos_unique, neg_unique)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:53:04.587083200Z",
     "start_time": "2023-09-21T19:53:04.580084100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь про потенциальные способы улучшить accuracy:\n",
    "\n",
    "- лемматизировать лексику в тональных словарях и тестовом сэмпле\n",
    "- убрать из тональных словарей стоп-слова\n",
    "- увеличить для тональных словарей порог частотности для входа\n",
    "\n",
    "а еще можно брать в тональный словарь не только токены, которые не встретились в текстах другой тональности, но еще и слова, у которых стат. значимо больше вхождений в тексты рассматриваемой тональности. но это я делать не буду)\n",
    "\n",
    "В общем, сперва пробую лемматизацию с pymorphy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "morph = pymorphy3.MorphAnalyzer(lang='ru')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:53:07.736900200Z",
     "start_time": "2023-09-21T19:53:07.423946300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [],
   "source": [
    "def lemmatize_texts(tokenized_texts: List[List[str]]) -> List[List[str]]:\n",
    "    \"\"\" Функция, которая лемматизирует токены в каждом из текстов\n",
    "    \"\"\"\n",
    "    return [[morph.parse(token)[0].normal_form for token in text] for text in tokenized_texts]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:53:08.822076200Z",
     "start_time": "2023-09-21T19:53:08.814590700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "pos_lemmatized = lemmatize_texts(pos_tokenized)\n",
    "neg_lemmatized = lemmatize_texts(neg_tokenized)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:53:11.186376600Z",
     "start_time": "2023-09-21T19:53:09.628592Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "pos_lemmatized_frequency = make_frequency_dicts(pos_lemmatized)\n",
    "neg_lemmatized_frequency = make_frequency_dicts(neg_lemmatized)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:53:11.194569500Z",
     "start_time": "2023-09-21T19:53:11.189312900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "pos_lemmatized_unique = make_set_of_unique_tonality_tokens(pos_lemmatized_frequency, neg_lemmatized_frequency, 2)\n",
    "neg_lemmatized_unique = make_set_of_unique_tonality_tokens(neg_lemmatized_frequency, pos_lemmatized_frequency, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:53:11.622996900Z",
     "start_time": "2023-09-21T19:53:11.608289Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "test_lemmatized = lemmatize_texts(test_tokenized)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:53:12.945305400Z",
     "start_time": "2023-09-21T19:53:12.784131600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Итак, лемматизация особо не изменила ситуацию (запускала несколько раз, оно то лучше, то хуже (слава рандому))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6666666666666666"
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(test_lemmatized, test_tonality, pos_lemmatized_unique, neg_lemmatized_unique)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:54:34.673377800Z",
     "start_time": "2023-09-21T19:54:34.657942200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь попробуем убрать стоп-слова из словарей"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"russian\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:54:36.342855100Z",
     "start_time": "2023-09-21T19:54:36.326351600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [],
   "source": [
    "pos_no_stopwords_unique = make_set_of_unique_tonality_tokens(pos_frequency, neg_frequency, 2, stop_words)\n",
    "neg_no_stopwords_unique = make_set_of_unique_tonality_tokens(neg_frequency, pos_frequency, 2, stop_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:54:37.275104400Z",
     "start_time": "2023-09-21T19:54:37.260472700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Тут то же, что и с лемматизацией. В зависимости от рандома то чуть лучше, то чуть хуже"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7"
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(test_tokenized, test_tonality, pos_no_stopwords_unique, neg_no_stopwords_unique)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:54:39.350827700Z",
     "start_time": "2023-09-21T19:54:39.324820800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Еще попробуем повысить порог частотности для токена, чтобы войти в словарь. Вот тут всегда точность ниже"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "pos_higher_freq_unique = make_set_of_unique_tonality_tokens(pos_frequency, neg_frequency, 3)\n",
    "neg_higher_freq_unique = make_set_of_unique_tonality_tokens(neg_frequency, pos_frequency, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:55:35.305563800Z",
     "start_time": "2023-09-21T19:55:35.301352700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5666666666666667"
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(test_tokenized, test_tonality, pos_higher_freq_unique, neg_higher_freq_unique)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:55:35.987369400Z",
     "start_time": "2023-09-21T19:55:35.980593900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
